{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2 - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from .csv, \n",
    "# split into X and y and  \n",
    "\n",
    "dataset = pd.read_csv('kc_house_data.csv')\n",
    "y = dataset[['price']]\n",
    "X = dataset.drop(['price', 'id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['house_age'] = X['date'].str.slice(0, 4).astype(int) -  X['yr_built']\n",
    "X = X.drop(['date','yr_built'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "house_age\n",
       "-1       12\n",
       " 0      430\n",
       " 1      285\n",
       " 2      174\n",
       " 3      165\n",
       "       ... \n",
       " 111     50\n",
       " 112     33\n",
       " 113     28\n",
       " 114     69\n",
       " 115     26\n",
       "Name: house_age, Length: 117, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['house_age'].groupby(X['house_age']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummies for categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21613, 102)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = ['waterfront', 'view','condition','grade', 'zipcode']\n",
    "X_dummies =  pd.get_dummies(X, columns = categorical_features, prefix = categorical_features, drop_first=True)\n",
    "X_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, input_train, input_test, output_train, output_test):\n",
    "    model.fit(input_train, output_train)\n",
    "\n",
    "    train_score = model.score(input_train,output_train)\n",
    "    train_preds = model.predict(input_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(output_train,train_preds))\n",
    "    print(\"Train :: R^2 = \" + \"%.2f\"%train_score + \" :: RMSE = \" + f'{train_rmse:,.0f}')\n",
    "\n",
    "    test_score = model.score(input_test,output_test)\n",
    "    test_preds = model.predict(input_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(output_test,test_preds))\n",
    "    print(\"Test  :: R^2 = \" + \"%.2f\"%test_score  + \" :: RMSE = \" + f'{test_rmse:,.0f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model from last week :: Linear Regression\n",
    "This version includes zipcode dummies and house_age as suggested by our study group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_dummies, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :: R^2 = 0.83 :: RMSE = 147,332\n",
      "Test  :: R^2 = 0.84 :: RMSE = 155,045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "evaluate(lr, x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :: R^2 = 0.83 :: RMSE = 147,334\n",
      "Test  :: R^2 = 0.84 :: RMSE = 155,012\n",
      "original y :: best lambda:3.76\n",
      "\n",
      "Train :: R^2 = 0.83 :: RMSE = 147,334\n",
      "Test  :: R^2 = 0.84 :: RMSE = 155,012\n",
      "centralised y :: best lambda:3.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sscale = StandardScaler()\n",
    "X_train_scale = sscale.fit_transform(x_train)\n",
    "X_test_scale = sscale.transform(x_test)\n",
    "y_train_scale = y_train - np.mean(y_train)\n",
    "y_test_scale = y_test - np.mean(y_train)\n",
    "\n",
    "a = np.geomspace(1,1000,100)\n",
    "ridge_cv = RidgeCV(alphas=a, cv=5)\n",
    "\n",
    "#not centralising y (Jacob)\n",
    "model = evaluate(ridge_cv,X_train_scale,X_test_scale,y_train, y_test)\n",
    "print(\"original y :: best lambda:\" + \"%.2f\"%model.alpha_ + \"\\n\")\n",
    "\n",
    "#centralising y (Carleton)\n",
    "model = evaluate(ridge_cv,X_train_scale,X_test_scale,y_train_scale, y_test_scale)\n",
    "print(\"centralised y :: best lambda:\" + \"%.2f\"%model.alpha_ + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation\n",
    "I've noticed that for random samples x_train, x_test, I have different best alphas.\n",
    "See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------   Iteration # 0    -------\n",
      "Train :: R^2 = 0.84 :: RMSE = 149,496\n",
      "Test  :: R^2 = 0.83 :: RMSE = 146,725\n",
      "original y ::best lambda:2.76\n",
      "\n",
      "-------   Iteration # 1    -------\n",
      "Train :: R^2 = 0.84 :: RMSE = 149,651\n",
      "Test  :: R^2 = 0.83 :: RMSE = 146,178\n",
      "original y ::best lambda:7.61\n",
      "\n",
      "-------   Iteration # 2    -------\n",
      "Train :: R^2 = 0.84 :: RMSE = 148,041\n",
      "Test  :: R^2 = 0.83 :: RMSE = 153,696\n",
      "original y ::best lambda:1.66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    print(\"-------   Iteration #\", n, \"   -------\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_dummies, y, test_size=0.2)\n",
    "    \n",
    "    sscale = StandardScaler()\n",
    "    X_train_scale = sscale.fit_transform(x_train)\n",
    "    X_test_scale = sscale.transform(x_test)\n",
    "    y_train_scale = y_train - np.mean(y_train)\n",
    "    y_test_scale = y_test - np.mean(y_train)\n",
    "\n",
    "    a = np.geomspace(1,500,50)\n",
    "    ridge_cv = RidgeCV(alphas=a, cv=5)\n",
    "\n",
    "    #not centralising y (Jacob)\n",
    "    model = evaluate(ridge_cv,X_train_scale,X_test_scale,y_train, y_test)\n",
    "    print(\"original y ::best lambda:\" + \"%.2f\"%model.alpha_ + \"\\n\")\n",
    "\n",
    "    '''\n",
    "    #centralising y (Carleton)\n",
    "    model = evaluate(ridge_cv,X_train_scale,X_test_scale,y_train_scale, y_test_scale)\n",
    "    print(\"centralised y :: best lambda:\" + \"%.2f\"%model.alpha_ + \"\\n\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "Ridge regression didn't improve the model. That might explain why we have different best alphas for each sample: they are irrelevant.\n",
    "\n",
    "I was already expecting no improvement, since Linear Regression model didn't show overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting Example:\n",
    "\n",
    "For the sake of learning, I need a model with overfitting, so I'm reducing the number of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = X_dummies[:500]\n",
    "y_small = y[:500]\n",
    "sscale_small = StandardScaler()\n",
    "X_small_scale = sscale_small.fit_transform(X_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.2, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :: R^2 = 0.89 :: RMSE = 116,926\n",
      "Test  :: R^2 = 0.35 :: RMSE = 287,987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "evaluate(lr, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overfitted\n",
    "\n",
    "Finally, Train score better than Test score. We do have overfitting!\n",
    "\n",
    "I can confirm this, running a cross validation over the whole data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 118,425    Test: 368,157\n"
     ]
    }
   ],
   "source": [
    "l = cross_validate(lr, X_small, y_small, cv=5, return_train_score=True, scoring='neg_root_mean_squared_error')\n",
    "train_score = -np.mean(l['train_score'])\n",
    "test_score = -np.mean(l['test_score'])\n",
    "print(\"Train:\", f'{train_score:,.0f}',\"   Test:\", f'{test_score:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Finding best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ ('scale', StandardScaler()), ('ridge', Ridge()) ])\n",
    "a = np.geomspace(100,200,500)\n",
    "params = {'ridge__alpha' : a}\n",
    "grid = GridSearchCV(pipe, param_grid=params, cv=5, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :: RMSE = 131,831\n",
      "Test  :: RMSE = 184,661\n"
     ]
    }
   ],
   "source": [
    "grid.fit(x_train,y_train)\n",
    "train_score = grid.score(x_train,y_train)\n",
    "print(\"Train :: RMSE = \" + f'{-train_score:,.0f}')\n",
    "\n",
    "test_score = grid.score(x_test,y_test)\n",
    "print(\"Test  :: RMSE = \" + f'{-test_score:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': 126.45966954704173}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Applying best alpha manually to double-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :: R^2 = 0.86 :: RMSE = 131,785\n",
      "Test  :: R^2 = 0.73 :: RMSE = 184,669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=126, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sscale = StandardScaler()\n",
    "X_train_scale = sscale.fit_transform(x_train)\n",
    "X_test_scale = sscale.transform(x_test)\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha=126)\n",
    "evaluate(ridge,X_train_scale,X_test_scale,y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final conclusion:\n",
    "Ridge regression improved the model for the toy-example of 500 observations.\n",
    "\n",
    "Linear Regression scores:\n",
    "\n",
    "    Train :: R^2 = 0.89 :: RMSE = 116,926\n",
    "    Test  :: R^2 = 0.35 :: RMSE = 287,987\n",
    "    \n",
    "    Cross Validation scores:\n",
    "    Train: 118,425    \n",
    "    Test: 368,157\n",
    "    \n",
    "Ridge Score (alpha = 126):\n",
    "\n",
    "    Train :: R^2 = 0.86 :: RMSE = 131,785\n",
    "    Test  :: R^2 = 0.73 :: RMSE = 184,669\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
